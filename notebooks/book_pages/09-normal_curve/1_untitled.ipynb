{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal curve\n",
    "\n",
    "- they started by discussing quantifiers about distribution shape\n",
    "    - how to quantify center, how to quantify width (don't go beyond)\n",
    "- then they say we see lots of real data that have same general shape, only differing on center and width\n",
    "- so we can define as normal curve to get these shapes\n",
    "- *why* do we see so much real data that matches this shape?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- look, we've seen histograms that look like this over and over again -- why is that? (intro - couple paragraphs) this is a mystery, but doesn't have to be that way\n",
    "    - is there a way we could have know a distribution would look like this?\n",
    "- distributions can have a lot of different shapes, don't all need to be bell curves\n",
    "    - include bimodal -- maybe eruptions of old faithful (durations)\n",
    "    - all of these are distributions, they can have different shapes\n",
    "    - how do we start to describe these shapes\n",
    "        - mean -- e.g. plot two things with same shape, but they differ by center\n",
    "        - emphasize point of view that the mean is a *property* of the histogram/distribution\n",
    "            - i.e. if the distribution doesn't change to much, then the mean won't change to much\n",
    "        - we want to measure how spread out a distribution is\n",
    "            - measure how far away from the center is\n",
    "            - but we can't just average out distance otherwise it cancels out\n",
    "            - we need to make this positive somehow\n",
    "                - one way is to just take absolute value\n",
    "                - this is fine, it works\n",
    "                - but usually we'll square it -- this gives us the variance\n",
    "                - why? it makes things easier to work with mathematically because squares are differentiable\n",
    "                - basically, both good, just var and stdv have better mathematical properties\n",
    "     - these don't tell us everything about the distribution...\n",
    "     - chebychev (?)\n",
    "- but, in the case of a bell curve, we can specify the shape exactly just by using the mean and standard deviation\n",
    "    - formula\n",
    "                \n",
    "                \n",
    "\n",
    "- first, let's start by describing the distribution\n",
    "- important to remember that mean and variation work on all distributions\n",
    "    - but they're not always useful\n",
    "    - e.g. counter examples like bimodal or saddle\n",
    "\n",
    "\n",
    "\n",
    "- we can keep chebychev\n",
    "\n",
    "\n",
    "- will need to talk about percentiles and cdf\n",
    "- how many within x% of the mean -- chebychev says this, but with normal assumption we get this\n",
    "\n",
    "- worth bringing up that this is an approximation, data might not exactly match normal curve, but close -- and if it's close enough we can leverage those assumptions\n",
    "\n",
    "\n",
    "\n",
    "central limit theorem\n",
    "- okay, we've seen this shape a lot, why is that?\n",
    "    - lots of natural measurements exhibit this bell curve (see lectures for examples)\n",
    "    - but it also always seems to show up when we start looking at sample means... why is that?\n",
    "\n",
    "- informally: the sum of a bunch of random variables sampled independently from the same distribution will be a normal curve\n",
    "    - from *any* shaped population\n",
    "    - can even give example with crazy pop\n",
    "    \n",
    "- clt for mean says you get curve with width stdv/sqrt(n), for sum says stdv\\*sqrt(n)\n",
    "    - this is difference between mean vs sum of a bunch of random variables iid\n",
    "\n",
    "- don't want to be too formal with definition\n",
    "- when we say iid, we can say independent samples from the same \n",
    "    \n",
    "- why does this show up in nature?\n",
    "    - there are *tons* of factors contributing to something in nature (like someone's height) -- since each of these factors are essentially independent, CLT applies and we get that normal curve!\n",
    "\n",
    "\n",
    "- Why useful: I want to make a confidence interval\n",
    "    - if I want to do this, I need to know how that stat is distrbuted\n",
    "    - before I used bootstrap, but if mean we can use CLT\n",
    "\n",
    "- we've introduced clt and shown that we get this shape\n",
    "    - there's no need to run bootstrap if we're interested in the mean\n",
    "    - now instead of having empirical, we have mathematical theoretical\n",
    "    - can still calculate percentiles and confidence intervals with some math equations\n",
    "    - so why is bootstrap still useful -- for stats that aren't the mean\n",
    "    \n",
    "    \n",
    "- everyone has intuition that when we get a larger sample, our sample mean will be more accurate\n",
    "    - can show that here -- the width of the distribution is a proxy for accuracy -- more likely to find a random sample statistic closer to the true mean\n",
    "- this segues into experiment design -- how many samples do I need to get a certain level of accuracy\n",
    "- remember that the mean is a property of a distribution, when I sample a small amount of numbers the resulting distribution can change a lot, but if I have lots of numbers then not to much changes about the distr\n",
    "\n",
    "\n",
    "- this whole time, we're making approximations\n",
    "    - e.g. we don't know the pop stdv, but it's probably close to the sample stdv\n",
    "    \n",
    "\n",
    "\n",
    "- statistics is a set of procedures that are reasonable\n",
    "    - they kinda just make sense. we can mathematically prove that a substitution (like sample stdv for pop stdv) is similar, but overall, they're just reasonable\n",
    "    \n",
    "    \n",
    "section to clarify misconceptions\n",
    "- e.g. \"central limit theorem vs bootstrap\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
